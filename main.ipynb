{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import sys, os, random, copy, collections, time, re, argparse\n",
    "sys.stdour = sys.stderr\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from utils import common\n",
    "from core import models, datasets\n",
    "from core.vocabularies import WordVocabularyWithEmbedding\n",
    "\n",
    "from utils.common import dotDict\n",
    "from main import Manager\n",
    "sys.stdout = sys.stderr\n",
    "args = dotDict({\n",
    "    'checkpoint_path': 'checkpoints/tmp',\n",
    "    'mode': 'train',\n",
    "    'config_path': 'configs/config',\n",
    "    'cleanup': True,\n",
    "    'log_file': None,\n",
    "    'interactive': True,\n",
    "})\n",
    "\n",
    "graph = tf.Graph()\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "sess.as_default()\n",
    "manager = Manager(args, sess)\n",
    "config = manager.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading word embeddings from dataset/embeddings/glove.840B.300d.txt.filtered...\n",
      "Done loading word embeddings.\n"
     ]
    }
   ],
   "source": [
    "vocab = WordVocabularyWithEmbedding(config.embeddings, vocab_size=config.vocab_size, lowercase=config.lowercase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[(['_BOS', 'the', 'insurance', 'value', 'of', '$', '2,500', '?'], ['2,500']), (['_BOS', 'birth', 'records', 'are', '$', '17.00', 'per', 'certified', 'copy', '.'], ['17.00']), (['_BOS', 'agradu', 'would', 'like', 'to', 'offer', 'a', '50', '%', 'off', 'coupon', 'for', '$', '200', 'less', 'than', 'half', 'the', 'cost', 'of', 'the', 'course', 'you', 'are'], ['200']), (['_BOS', 'prices', 'start', 'at', '$', '29.95', 'and', 'vary', 'slightly', 'depending', 'on', 'the', 'internet', 'service', 'provider', '.'], ['29.95']), (['_BOS', 'congress', 'will', 'appropriate', '$', '250', 'billion', 'to', 'begin', 'with', ',', 'another', '$', '100', 'billion', 'can', 'be', 'accessed', '.'], ['250', 'billion']), (['_BOS', 'only', '$', '9', 'us', 'each', '6', 'months', '.'], ['9']), (['_BOS', '$', '180', 'a', 'day', 'during', 'the', 'dry', 'months', '.'], ['180']), (['_BOS', 'the', 'cost', 'is', '$', '7.95', 'per', 'year', '.'], ['7.95']), (['_BOS', 'at', 'the', 'end', 'of', '2003', ',', 'the', 'canadian', 'dollar', 'was', 'worth', 'about', '70', 'cents', 'on', 'the', 'us', 'dollar', '.'], ['70']), (['_BOS', 'in', 'asia', 'they', 'probably', 'pay', 'at', 'least', '$', '12', '.gal', 'for', 'gas', '(', 'we', 'whine', ',', 'cry', 'and', 'get', 'angry', ')'], ['12']), (['_BOS', 'would', 'anyone', 'pay', '$', '1.25', 'or', 'even', '$', '1.10', 'for', 'a', 'song', 'now', 'that', 'the', 'price', 'for', 'a', 'tv', 'show', 'is', 'set', 'at', '$', '2', '?'], ['1.25']), (['_BOS', 'youl', 'need', 'to', 'make', 'a', 'minimum', 'downpayment', 'of', '$', '750', 'or', '5', '%', 'of', 'your', 'assets', ',', 'whichever', 'is', 'greater', '.'], ['750']), (['_BOS', 'initially', 'there', 'is', 'a', 'one', 'time', 'fee', 'of', '$', '289', ',', 'this', 'is', 'your', 'new', 'member', 'fee', '.'], ['289']), (['_BOS', 'in', '30', 'minute', 'meters', ':', '$', '.', '25', 'per', '30', 'minutes'], ['.', '25']), (['_BOS', 'the', 'cost', 'of', 'a', 'payday', 'loan', 'is', 'usually', 'quoted', 'as', 'a', 'fee', 'per', '$', '100', 'borrowed', 'and', 'the', 'pay', 'day', 'loan', 'must', 'be'], ['100']), (['_BOS', 'if', 'the', 'conference', 'expenses', 'cost', 'more', 'than', '$', '750', ',', 'you', 'must', 'find', 'another', 'funding', 'source', '.'], ['750']), (['_BOS', 'listings', 'start', 'at', '$', '95', '/', 'job', '.'], ['95']), (['_BOS', '$', '60', 'million', '.'], ['60', 'million']), (['_BOS', 'you', 'have', 'no', 'idea', 'what', 'the', 'final', 'tag', 'would', 'have', 'been', 'on', 'the', '$', '700', 'billion', '.'], ['700', 'billion']), (['_BOS', '$', '20', 'to', '$', '25', 'for', 'the', 'glasses', '.'], ['20']), (['_BOS', 'national', 'costs', 'for', 'a', 'neuromuscular', 'therapist', 'range', 'from', '$', '70', 'to', '$', '135', 'per', 'hour', '.'], ['70']), (['_BOS', 'since', 'the', 'iraq', 'war', 'began', '5', 'years', 'ago', 'the', 'total', 'cost', 'is', 'around', '$', '620', 'billion', '.'], ['620', 'billion']), (['_BOS', 'a', 'round-trip', 'flight', 'from', 'miami', 'to', 'bolivia', 'can', 'range', 'from', '$', '700', '-', '$', '950', 'round', 'trip', '.'], ['700']), (['_BOS', 'most', 'professional', 'drums', 'teachers', 'charge', 'between', '$', '20', '-', '$', '60', 'per', 'hour', '.'], ['20']), (['_BOS', 'the', 'answer', 'is', 'between', '$', '35', '(', 'poor', 'quality', 'stuff', ')', 'to', '$', '100', 'for', 'a', 'good', 'pound', 'of', 'medical', 'marihuana', '.'], ['35'])]\n"
     ]
    }
   ],
   "source": [
    "dataset_type = getattr(datasets, config.dataset_type)\n",
    "dataset = dataset_type(config.dataset_path.test, vocab)\n",
    "sys.stdout = sys.stderr\n",
    "print dataset.raw_data\n",
    "# print config.input_max_len, config.output_max_len\n",
    "# batches = dataset.get_batch(2)\n",
    "# for i, batch in enumerate(batches):\n",
    "#     print batch.sources.shape\n",
    "#     print batch.targets.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, targets = dataset.symbolized\n",
    "sys.stdout = sys.stderr\n",
    "from core.vocabularies import BOS_ID\n",
    "\n",
    "sources =  tf.keras.preprocessing.sequence.pad_sequences(sources, maxlen=config.input_max_len+1, padding='post', truncating='post', value=BOS_ID)\n",
    "targets =  tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=config.output_max_len, padding='post', truncating='post', value=BOS_ID)\n",
    "sources = infer_dataset = tf.data.Dataset.from_tensor_slices(sources)\n",
    "targets = infer_dataset = tf.data.Dataset.from_tensor_slices(targets)\n",
    "dataset = tf.data.Dataset.zip((sources, targets))\n",
    "print dataset.batch(32)\n",
    "print dir(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "    model = manager.create_model(manager.config, vocab)\n",
    "    #dataset = dataset_type(config.dataset_path, self.vocab)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.stdout = sys.stderr\n",
    "from six.moves import xrange # pylint: disable=redefined-builtin\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.ops.rnn_cell_impl import _linear # tf1.4\n",
    "encoder_inputs = model.e_inputs_emb\n",
    "decoder_inputs =model.d_inputs\n",
    "attention_states = model.attention_states\n",
    "initial_state = model.e_state\n",
    "cell = model.d_cell\n",
    "feed_prev=True\n",
    "dtype=dtypes.float32\n",
    "scope=None\n",
    "print encoder_inputs\n",
    "print decoder_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.variable_scope(scope or \"point_decoder\", reuse=tf.AUTO_REUSE):\n",
    "    print 'encoder_inputs',encoder_inputs\n",
    "    print 'decoder_inputs', decoder_inputs\n",
    "    print 'attention_states', attention_states\n",
    "    attn_length = attention_states.get_shape()[1].value\n",
    "    attn_size = attention_states.get_shape()[2].value\n",
    "    states = [initial_state]\n",
    "    outputs = []\n",
    "\n",
    "    attnw = tf.get_variable(\"AttnW\", [1, attn_size, attn_size])\n",
    "    attention_states = tf.nn.conv1d(attention_states, attnw, 1, 'SAME')\n",
    "    attnv = tf.get_variable(\"AttnV\", [attn_size])\n",
    "\n",
    "    def attention(output):\n",
    "        print 'attention_states', attention_states\n",
    "        print 'output', output\n",
    "        y = _linear(output, attn_size, True)\n",
    "        y = tf.reshape(y, [-1, 1, attn_size])\n",
    "        attention_vectors = tf.nn.softmax(tf.reduce_sum(attnv * tf.tanh(y + attention_states), axis=2))\n",
    "        return attention_vectors\n",
    "\n",
    "    for i, d in enumerate(tf.unstack(decoder_inputs, axis=1)):\n",
    "        print i\n",
    "        if i > 0:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        pointed_idx = d\n",
    "        # in testing, inputs to decoder won't be used except the first one.\n",
    "        if feed_prev and i > 0:\n",
    "            # take argmax, convert the pointed index into one-hot, and get the pointed encoder_inputs by multiplying and reduce_sum.\n",
    "            pointed_idx =  tf.argmax(output, axis=1)\n",
    "        \n",
    "        pointed_idx = tf.reshape(tf.one_hot(pointed_idx, depth=attn_length), [-1, attn_length, 1]) \n",
    "        inp = tf.reduce_sum(encoder_inputs * pointed_idx, axis=1) \n",
    "        print 'pointed_idx',pointed_idx\n",
    "        print 'next_inp', inp\n",
    "        output, state = cell(inp, states[-1])\n",
    "        output = attention(output)\n",
    "        states.append(state)\n",
    "        outputs.append(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = getattr(datasets, config.dataset_type)\n",
    "dataset = dataset_type(config.dataset_path, vocab)\n",
    "sources, targets = dataset.symbolized\n",
    "sources = sources[:3]\n",
    "sources = [' '.join([str(x) for x in s]) for s in sources]\n",
    "\n",
    "print sources\n",
    "dataset = dataset.map(lambda string: tf.string_split([string]).values)\n",
    "# sources = tf.keras.preprocessing.sequence.pad_sequences(sources, padding='post', truncating='post')\n",
    "# print sources \n",
    "# sources = sources.map(lambda words, size: (table.lookup(words), size))\n",
    "# print sources\n",
    "\n",
    "#print tf.data.Dataset.from_tensor_slices(sources)\n",
    "#print tf.data.Dataset(sources)\n",
    "#print tf.keras.preprocessing.sequence.pad_sequences(dataset.symbolized)\n",
    "#tf.data.Dataset.zip(dataset.symbolized)\n",
    "# for i, (source, target) in enumerate(dataset.symbolized):\n",
    "#     print source, target\n",
    "#     print dataset.original_sources[i], [dataset.original_sources[i][t] for t in target]\n",
    "#     if dataset.targets[i] != ['-'] and not ids:\n",
    "#         print dataset.texts[i]\n",
    "#         print dataset.values[i].split(':')[0]\n",
    "#         print i, dataset.original_sources[i], dataset.targets[i], ids\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens2ids(words):\n",
    "    return [vocab.token2id(w) for w in words]\n",
    "\n",
    "infer_batch = tf.placeholder(tf.string, shape=(None,))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(infer_batch)\n",
    "dataset = dataset.map(lambda string: tf.string_split([string]).values)\n",
    "dataset = dataset.map(lambda words: (words, tf.size(words)))\n",
    "dataset = dataset.map(lambda words, size: (tokens2ids(words), size))\n",
    "print dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-1.5",
   "language": "python",
   "name": "tf-1.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
